# OpenAI API (default)
LLM_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini
LLM_API_KEY=sk-your-openai-api-key

# Ollama (local server)
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_MODEL=llama3.2
# LLM_API_KEY=ollama

# Mistral AI
# LLM_BASE_URL=https://api.mistral.ai/v1
# LLM_MODEL=mistral-small-latest
# LLM_API_KEY=your-mistral-api-key

# Google Gemini via OpenAI-compatible endpoint
# LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai
# LLM_MODEL=gemini-1.5-flash
# LLM_API_KEY=your-google-api-key

# Anthropic Claude (via compatible proxy)
# LLM_BASE_URL=https://api.anthropic.com/v1
# LLM_MODEL=claude-3-haiku-20240307
# LLM_API_KEY=your-anthropic-api-key

# LocalAI (self-hosted)
# LLM_BASE_URL=http://localhost:8080/v1
# LLM_MODEL=ggml-gpt4all-j
# LLM_API_KEY=not-needed

# Groq
# LLM_BASE_URL=https://api.groq.com/openai/v1
# LLM_MODEL=llama-3.1-70b-versatile
# LLM_API_KEY=your-groq-api-key

# LLM System Prompt (optional)
# LLM_SYSTEM_PROMPT="You are a helpful AI assistant. Be polite and informative in your responses."

# Additional model parameters
# LLM_TEMPERATURE=0.7
# LLM_MAX_TOKENS=1000

# Request timeout (seconds)
REQUEST_TIMEOUT=30.0